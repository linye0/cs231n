# CS231n

## Lecture02 Image Classification

![image-20221016193756115](E:\TyporaPicture\image-20221016193756115.png)

难点：

1. 计算机中没有cat，只有pixels，每个pixels有自己的颜色属性
2. 光线的变化，姿势的变化，甚至可能无法看到完整的cat……

 ![image-20221016194141852](E:\TyporaPicture\image-20221016194141852.png)

边缘检测：算法太脆弱了，拓展性差

![image-20221016194228948](E:\TyporaPicture\image-20221016194228948.png)

用数据集进行训练，建立模型，然后预测

two functions: 1. train 2. predict

```python
def train(images, labels):
    # Machine learning
    return model

def predict(model, test_images):
    # Use model to predict labels
    return test_labels
```

Example Dataset: **CIFAR10**

### Nearest Neighbour Classifier

![image-20221016194737761](E:/TyporaPicture/image-20221016194737761.png)

![image-20221016195210280](E:/TyporaPicture/image-20221016195210280.png)

代码实现：

Xtr是训练图像数据5000\*32\*32\*3（5000张图，32*32像素，3通道），Ytr是训练图像的标签数组（5000），Xte是测试图像数据，Yte是测试图像标签（真实标签）

```python
Xtr, Ytr, Xte, Yte = load_CIFAR10('data/cifar10/') # a magic function we provide
# flatten out all images to be one-dimensional
Xtr_rows = Xtr.reshape(Xtr.shape[0], 32 * 32 * 3) # Xtr_rows becomes 50000 x 3072
Xte_rows = Xte.reshape(Xte.shape[0], 32 * 32 * 3) # Xte_rows becomes 10000 x 3072
```

预测过程图示：

![image-20221020102006288](E:/TyporaPicture/image-20221020102006288.png)

预测结果代码：

```python
nn = NearestNeighbor() # create a Nearest Neighbor classifier class
nn.train(Xtr_rows, Ytr) # train the classifier on the training images and labels
Yte_predict = nn.predict(Xte_rows) # predict labels on the test images
# and now print the classification accuracy, which is the average number
# of examples that are correctly predicted (i.e. label matches)
print 'accuracy: %f' % ( np.mean(Yte_predict == Yte) )
```

```python
import numpy as np

class NearestNeighbor(object):
  def __init__(self):
    pass

  def train(self, X, y):
    """ X is N x D where each row is an example. Y is 1-dimension of size N """
    # the nearest neighbor classifier simply remembers all the training data
    self.Xtr = X
    self.ytr = y

  def predict(self, X):
    """ X is N x D where each row is an example we wish to predict label for """
    num_test = X.shape[0]
    # lets make sure that the output type matches the input type
    Ypred = np.zeros(num_test, dtype = self.ytr.dtype)

    # loop over all test rows
    for i in range(num_test):
      # find the nearest training image to the i'th test image
      # using the L1 distance (sum of absolute value differences)
      distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1) # tile?
      min_index = np.argmin(distances) # get the index with smallest distance
      Ypred[i] = self.ytr[min_index] # predict the label of the nearest example

    return Ypred
```

距离的选择：

评估像素点的距离，除了用绝对值距离，还可以用平方根距离
$$
d_2(I_1,I_2)=\sqrt{\sum_p{(I_1^p-I_2^p)^2}}
$$
只需把计算distance的代码改为：

```python
distances = np.sqrt(np.sum(np.square(self.Xtr - X[i,:]), axis = 1))
```

**K-Nearest Neighbour Classifier**

![image-20221016195508984](E:/TyporaPicture/image-20221016195508984.png)

用K-Nearest Neighbor来去除噪点

![image-20221016195820905](E:/TyporaPicture/image-20221016195820905.png)

![image-20221016200044839](E:/TyporaPicture/image-20221016200044839.png)

KNN的代码实现：

```python
# assume we have Xtr_rows, Ytr, Xte_rows, Yte as before
# recall Xtr_rows is 50,000 x 3072 matrix
Xval_rows = Xtr_rows[:1000, :] # take first 1000 for validation
Yval = Ytr[:1000]
Xtr_rows = Xtr_rows[1000:, :] # keep last 49,000 for train
Ytr = Ytr[1000:]

# find hyperparameters that work best on the validation set
validation_accuracies = []
for k in [1, 3, 5, 10, 20, 50, 100]:

  # use a particular value of k and evaluation on validation data
  nn = NearestNeighbor()
  nn.train(Xtr_rows, Ytr)
  # here we assume a modified NearestNeighbor class that can take a k as input
  Yval_predict = nn.predict(Xval_rows, k = k)
  acc = np.mean(Yval_predict == Yval)
  print 'accuracy: %f' % (acc,)

  # keep track of what works on the validation set
  validation_accuracies.append((k, acc))
```

![image-20221016201250938](E:/TyporaPicture/image-20221016201250938.png)

Cross-validation 在样本量较少的时候，可以使用交叉验证法，比如将总样本五等分，每次用其中的四份训练模型，用剩下的一份进行验证，以下是使用交叉验证法和KNN所作出的精确度图像：

![7](E:/TyporaPicture/image-20221016201728144.png)

***Nearest Neighbors的方法并不好***

### Linear Classifier

![image-20221016202849568](E:/TyporaPicture/image-20221016202849568.png)

![image-20221016203100720](E:/TyporaPicture/image-20221016203100720.png)

Linear Classifier 会为每种物体学习一个模板，然后对样例进行筛选，但是对于每一个类别，只能学习一种模板

![image-20221016203530545](E:/TyporaPicture/image-20221016203530545.png)

## Lecture03: Loss Function and Optimation

### Loss Function

#### SVM Loss Function

![image-20221017115149871](E:/TyporaPicture/image-20221017115149871.png)
$$
L = \frac{1}{N} \sum_{i}L_i(f(x_i,W), y_i)
$$
*f是linear算法，y~i~是目标标签（integer），L~i~是自定义的Loss Function，用于计算偏差值，N是所有的样本数，L是最后得到的偏差值*

![image-20221017131641265](E:/TyporaPicture/image-20221017131641265.png)

![image-20221017132110712](E:/TyporaPicture/image-20221017132110712.png)

计算L~i~损失值，以上图为例，在第一列中，正确的预测对象是cat，得分(S~yi~)是3.2分，而car的得分(S~j~)是5.1分，可得car所贡献的损失值为5.1 - 3.2 + 1 = 2.9分，而frog的得分是-1.7分，可得frog共享的损失值为-1.7 - 3.2 + 1 = -3.9 < 0，因此将frog的损失值调整为0，总共损失值L~i~ = 2.9 + 0 = 2.9，其它两列同理。

```python
# Multiclass SVM Loss: Example code
def L_i_vectorized(x, y, W):
    scores = W.dot(x)
    margins = np.maxmum(0, scores - scores[y] + 1)
    margins[y] = 0
    loss_i = np.sum(margins)
    return lose_i
```

![image-20221017134154892](E:/TyporaPicture/image-20221017134154892.png)

对于一组训练数据，最后得到的最优模板W是不唯一的，例如假如W~0~是一个符合L~i~ = 0的模板，那么2W显然也是符合条件的模板，因此可以引入R(W)，来选取最简单的模板

![image-20221017134645710](E:/TyporaPicture/image-20221017134645710.png)

R(W)正则化的目的是减轻模型的复杂度，而不是拟合数据

代码实现（没有加入正则化）：

```python
def L_i(x, y, W):
  """
  unvectorized version. Compute the multiclass svm loss for a single example (x,y)
  - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)
    with an appended bias dimension in the 3073-rd position (i.e. bias trick)
  - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)
  - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)
  """
  delta = 1.0 # see notes about delta later in this section
  scores = W.dot(x) # scores becomes of size 10 x 1, the scores for each class
  correct_class_score = scores[y]
  D = W.shape[0] # number of classes, e.g. 10
  loss_i = 0.0
  for j in range(D): # iterate over all wrong classes
    if j == y:
      # skip for the true class to only loop over incorrect classes
      continue
    # accumulate loss for the i-th example
    loss_i += max(0, scores[j] - correct_class_score + delta)
  return loss_i

def L_i_vectorized(x, y, W):
  """
  A faster half-vectorized implementation. half-vectorized
  refers to the fact that for a single example the implementation contains
  no for loops, but there is still one loop over the examples (outside this function)
  """
  delta = 1.0
  scores = W.dot(x)
  # compute the margins for all classes in one vector operation
  margins = np.maximum(0, scores - scores[y] + delta)
  # on y-th position scores[y] - scores[y] canceled and gave delta. We want
  # to ignore the y-th position and only consider margin on max wrong class
  margins[y] = 0
  loss_i = np.sum(margins)
  return loss_i

def L(X, y, W):
  """
  fully-vectorized implementation :
  - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10)
  - y is array of integers specifying correct class (e.g. 50,000-D array)
  - W are weights (e.g. 10 x 3073)
  """
  # evaluate loss over all examples in X without using any for loops
  # left as exercise to reader in the assignment
```

#### Softmax Loss Function

![image-20221017140731227](E:/TyporaPicture/image-20221017140731227.png)

根据信息论得到的式子：**Information theory view**. The *cross-entropy* between a “true” distribution *p* and an estimated distribution *q* is defined as:
$$
H(p,q)=-\sum_{x}p(x)logq(x)
$$
![image-20221017201538341](E:/TyporaPicture/image-20221017201538341.png)

在实际处理中，会发现有时候e^fyi^和∑~j~e^fj^会很大，这时候可以用下式来处理：
$$
\frac{e^{f_{yi}}}{\sum_{j}e^{f_{yj}}}=\frac{Ce^{f_{yi}}}{\sum_{j}e^{f_{yj}}}=\frac{e^{f_{yi}+logC}}{\sum_{j}e^{f_{yj}+logC}}
$$
通常挑选C为logC = -max~j~f~j~

```python
f = np.array([123, 456, 789]) # example with 3 classes and each having large scores
p = np.exp(f) / np.sum(np.exp(f)) # Bad: Numeric problem, potential blowup

# instead: first shift the values of f so that the highest number is 0:
f -= np.max(f) # f becomes [-666, -333, 0]
p = np.exp(f) / np.sum(np.exp(f)) # safe to do, gives the correct answer
```

![image-20221017141529968](E:/TyporaPicture/image-20221017141529968.png)

![image-20221017143109587](E:/TyporaPicture/image-20221017143109587.png)

### Optimization

优化的步骤：从某个方法开始，不断迭代，逐步对方法进行改进

#### Random Search

> A first very bad idea solution

随机化W模板，每次都计算损失值，重复n次，取损失值最小的一种模板（非常差的方法，只能达到大约15%的精确度）

```python
# assume X_train is the data where each column is an example (e.g. 3073 x 50,000)
# assume Y_train are the labels (e.g. 1D array of 50,000)
# assume the function L evaluates the loss function

bestloss = float("inf") # Python assigns the highest possible float value
for num in range(1000):
  W = np.random.randn(10, 3073) * 0.0001 # generate random parameters
  loss = L(X_train, Y_train, W) # get the loss over the entire training set
  # 这里使用的L是上面要求自己编写的L，可以一次性实现多张照片的损失值计算
  if loss < bestloss: # keep track of the best solution
    bestloss = loss
    bestW = W
  print 'in attempt %d the loss was %f, best %f' % (num, loss, bestloss)

# prints:
# in attempt 0 the loss was 9.401632, best 9.401632
# in attempt 1 the loss was 8.959668, best 8.959668
# in attempt 2 the loss was 9.044034, best 8.959668
# in attempt 3 the loss was 9.278948, best 8.959668
# in attempt 4 the loss was 8.857370, best 8.857370
# in attempt 5 the loss was 8.943151, best 8.857370
# in attempt 6 the loss was 8.605604, best 8.605604
# ... (trunctated: continues for 1000 lines)
```

```python
# Test accuracy
# Assume X_test is [3073 x 10000], Y_test [10000 x 1]
scores = Wbest.dot(Xte_cols) # 10 x 10000, the class scores for all test examples
# find the index with max score in each column (the predicted class)
Yte_predict = np.argmax(scores, axis = 0)
# and calculate accuracy (fraction of predictions that are correct)
np.mean(Yte_predict == Yte)
# returns 0.1555
```

#### Random Local Search

> also bad, but a little better

不是随机取点，而是随机迈步，就像下山，你没有办法一眼看到下山的路，但是有办法通过脚下的梯度来感受目前该往那边走，最后到达山底（微调矩阵W，使其变成W+λW，如果W+λW的表现更好，就采用W+λW）

```python
W = np.random.randn(10, 3073) * 0.001 # generate random starting W
bestloss = float("inf")
for i in range(1000):
  step_size = 0.0001
  Wtry = W + np.random.randn(10, 3073) * step_size
  loss = L(Xtr_cols, Ytr, Wtry)
  if loss < bestloss:
    W = Wtry
    bestloss = loss
  print 'iter %d loss is %f' % (i, bestloss)
```

算法准确度可以达到21.4%

#### Following the Gradient

$$
\frac{df(x)}{dx} = \lim_{h\to0}\frac{f(x+h)-f(x)}{h}
$$

有两种计算梯度的方法，第一种是**numerical gradient**，第二种是**analytic gradient**

##### Numerical Gradient

计算梯度函数：

```python
def eval_numerical_gradient(f, x):
  """
  a naive implementation of numerical gradient of f at x
  - f should be a function that takes a single argument
  - x is the point (numpy array) to evaluate the gradient at
  """

  fx = f(x) # evaluate function value at original point
  grad = np.zeros(x.shape)
  h = 0.00001

  # iterate over all indexes in x
  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])
  while not it.finished:
	# multi_index为it在矩阵中的当前位置(如<0, 0>, <1, 0>, <2, 3>)
    # evaluate function at x+h
    ix = it.multi_index
    old_value = x[ix]
    x[ix] = old_value + h # increment by h
    fxh = f(x) # evalute f(x + h)
    x[ix] = old_value # restore to previous value (very important!)
	# 算法实现每次循环只对W模板中的一个元素做微调
    # compute the partial derivative
    grad[ix] = (fxh - fx) / h # the slope
    it.iternext() # step to next dimension

  return grad
```

算出在CIFAR10训练集下的梯度：

```python
# to use the generic code above we want a function that takes a single argument
# (the weights in our case) so we close over X_train and Y_train
def CIFAR10_loss_fun(W):
  return L(X_train, Y_train, W)

W = np.random.rand(10, 3073) * 0.001 # random weight vector
df = eval_numerical_gradient(CIFAR10_loss_fun, W) # get the gradient
```

根据算出的梯度实现优化，这里应该仅为一次优化，因为每次优化都要重新计算梯度，Step循环是为了测出最好的步长（也叫 learing rate）是多少：

```python
loss_original = CIFAR10_loss_fun(W) # the original loss
print 'original loss: %f' % (loss_original, )

# lets see the effect of multiple step sizes
for step_size_log in [-10, -9, -8, -7, -6, -5,-4,-3,-2,-1]:
  step_size = 10 ** step_size_log
  W_new = W - step_size * df # new position in the weight space
  loss_new = CIFAR10_loss_fun(W_new)
  print 'for step size %f new loss: %f' % (step_size, loss_new)

# prints:
# original loss: 2.200718
# for step size 1.000000e-10 new loss: 2.200652
# for step size 1.000000e-09 new loss: 2.200057
# for step size 1.000000e-08 new loss: 2.194116
# for step size 1.000000e-07 new loss: 2.135493
# for step size 1.000000e-06 new loss: 1.647802
# for step size 1.000000e-05 new loss: 2.844355
# for step size 1.000000e-04 new loss: 25.558142
# for step size 1.000000e-03 new loss: 254.086573
# for step size 1.000000e-02 new loss: 2539.370888
# for step size 1.000000e-01 new loss: 25392.214036
```

![image-20221017210359030](E:/TyporaPicture/image-20221017210359030.png)

以下为Numerical Gradient的Lecture笔记：

![image-20221017145602247](E:/TyporaPicture/image-20221017145602247.png)

有限差分法：每次给因变量加上一个微小量，观察Loss Function算出的偏差值w，最右侧矩阵是dw/dx~i~，代表某一因变量对应的梯度

![image-20221017150042932](E:/TyporaPicture/image-20221017150042932.png)

梯度下降算法：慢，但是简单，可以用来检验

![image-20221017150448495](E:/TyporaPicture/image-20221017150448495.png)

图像化，黑色线条是梯度下降算法

#### Analytic Gradient

Analytic Gradient 算法相比 Numerical Gradient 有更高的效率，但是容易出错（more error prone?），所以在实际应用的时候，经常用 Numerical 法来检验 Analytic 算法

![image-20221017211739912](E:/TyporaPicture/image-20221017211739912.png)

**但是为什么？**就是基础的求导，只不过把自变量变成了Wy~i~

记得最后要正则化

#### Result: Gradient Descent

```python
# Vanilla Gradient Descent

while True:
  weights_grad = evaluate_gradient(loss_fun, data, weights)
  weights += - step_size * weights_grad # perform parameter update
```

**Mini-batch gradient descent**

只采取一小部分样本用于改进模板W

```python
# Vanilla Minibatch Gradient Descent

while True:
  data_batch = sample_training_data(data, 256) # sample 256 examples
  weights_grad = evaluate_gradient(loss_fun, data_batch, weights)
  weights += - step_size * weights_grad # perform parameter update
```

> The size of the mini-batch is a hyperparameter but it is not very common to cross-validate it. It is usually based on memory constraints (if  any), or set to some value, e.g. 32, 64 or 128. We use powers of 2 in  practice because many vectorized operation implementations work faster  when their inputs are sized in powers of 2.

![image-20221017150735354](E:/TyporaPicture/image-20221017150735354.png)

## Lecture04: Introduction to Neural Net

### 梯度回溯算法

#### 计算梯度的方法（链式法则）

$$
\frac{df}{dx} = \sum_{i}\frac{df}{dq_i}\frac{dq_i}{dx}
$$
![image-20221018164023725](E:/TyporaPicture/image-20221018164023725.png)

![image-20221018164459639](E:/TyporaPicture/image-20221018164459639.png)

```python
# set some inputs
x = -2; y = 5; z = -4

# perform the forward pass
q = x + y # q becomes 3
f = q * z # f becomes -12

# perform the backward pass (backpropagation) in reverse order:
# first backprop through f = q * z
dfdz = q # df/dz = q, so gradient on z becomes 3
dfdq = z # df/dq = z, so gradient on q becomes -4
dqdx = 1.0
dqdy = 1.0
# now backprop through q = x + y
dfdx = dfdq * dqdx  # The multiplication here is the chain rule!
dfdy = dfdq * dqdy  
```

#### 一个较为复杂的例子

![image-20221018165556127](E:/TyporaPicture/image-20221018165556127.png)

也可以把几个步骤集合成一个sogmoid function：

![image-20221018170246195](E:/TyporaPicture/image-20221018170246195.png)

```python
w = [2,-3,-3] # assume some random weights and data
x = [-1, -2]

# forward pass
dot = w[0]*x[0] + w[1]*x[1] + w[2]
f = 1.0 / (1 + math.exp(-dot)) # sigmoid function

# backward pass through the neuron (backpropagation)
ddot = (1 - f) * f # gradient on dot variable, using the sigmoid gradient derivation
dx = [w[0] * ddot, w[1] * ddot] # backprop into x
dw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot] # backprop into w
# we're done! we have the gradients on the inputs to the circuit
```

Max gate 和 Mul gate:

Max gate可以看作一个路径转换器，会把下游的梯度值传递到上游输入值最大的一段，mul gate可以看作一个放缩器，会把下游的梯度值传递到上游并且用另一端的输出进行缩放：
![image-20221018170729628](E:/TyporaPicture/image-20221018170729628.png)

#### 一个更为复杂的例子

![image-20221019155805456](E:/TyporaPicture/image-20221019155805456.png)

求f(x, y)的梯度：

forward pass of expression (计算结果)：

```python
x = 3 # example values
y = -4

# forward pass
sigy = 1.0 / (1 + math.exp(-y)) # sigmoid in numerator   #(1)
num = x + sigy # numerator                               #(2)
sigx = 1.0 / (1 + math.exp(-x)) # sigmoid in denominator #(3)
xpy = x + y                                              #(4)
xpysqr = xpy**2                                          #(5)
den = sigx + xpysqr # denominator                        #(6)
invden = 1.0 / den                                       #(7)
f = num * invden # done!                                 #(8)
```

backprop (梯度回溯)：

```python
# backprop f = num * invden
dnum = invden # gradient on numerator                             #(8)
dinvden = num                                                     #(8)
# backprop invden = 1.0 / den 
dden = (-1.0 / (den**2)) * dinvden                                #(7)
# backprop den = sigx + xpysqr
dsigx = (1) * dden                                                #(6)
dxpysqr = (1) * dden                                              #(6)
# backprop xpysqr = xpy**2
dxpy = (2 * xpy) * dxpysqr                                        #(5)
# backprop xpy = x + y
dx = (1) * dxpy                                                   #(4)
dy = (1) * dxpy                                                   #(4)
# backprop sigx = 1.0 / (1 + math.exp(-x))
dx += ((1 - sigx) * sigx) * dsigx # Notice += !! See notes below  #(3)
# backprop num = x + sigy
dx += (1) * dnum                                                  #(2)
dsigy = (1) * dnum                                                #(2)
# backprop sigy = 1.0 / (1 + math.exp(-y))
dy += ((1 - sigy) * sigy) * dsigy                                 #(1)
# done! phew
```

注意，在回溯的时候如果之前已经出现过一次dx，之后的dx要改用+=而不是=，因为最终的df/dx是每个df/dx的总和。

多维的情况（矩阵求导）：			

![image-20221018214500884](E:/TyporaPicture/image-20221018214500884.png)

附：矩阵乘法的梯度计算

![image-20221019152646615](E:/TyporaPicture/image-20221019152646615.png)

### Neural Network

![image-20221019162248309](E:/TyporaPicture/image-20221019162248309.png)

![image-20221019162829208](E:/TyporaPicture/image-20221019162829208.png)

![image-20221019162916209](E:/TyporaPicture/image-20221019162916209.png)

![image-20221019163006348](E:/TyporaPicture/image-20221019163006348.png)

#### Notebook 部分

##### Notes 1

**Quick intro**

原来的评分体系为s = Wx（其中W为10\*3072的模板，x为，比如说100个列向量，每个列向量长度为3072，代表有100张3072像素的图片），可以引入一个过渡变量，使其变为s = W~2~max(0, W~1~x)，其中W~1~是一个中间矩阵，比如一个尺寸为100\*3072的矩阵，来对x做中间处理，max使中间过程称为非线性的，*”这个非线性函数有多种选择，后续将会学到。但这个形式是一个最常用的选择，它就是简单地设置阈值，将所有小于0的值变成0“*。同理，三层神经网络可以是s = W~3~max(0, W~2~max(0, W~1~x))。

**Modeling one neuron**

